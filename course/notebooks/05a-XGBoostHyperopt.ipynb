{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436e1229-b030-48f4-8422-e7698990b524",
   "metadata": {},
   "source": [
    "> ## ❓Questions\n",
    "> - What are some newer approaches to ML?\n",
    "> - What are their pros and cons?\n",
    ">\n",
    "> ## ☑︎ Objectives\n",
    "> - Explore how to optimise ML models with\n",
    ">   a much larger number of parameters\n",
    "> - Learn strategies for dealing with large models and\n",
    ">   large parameter spaces\n",
    "\n",
    "# Gradient boosting methods: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329c5b22-4676-40fe-a567-268c8d37e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when delivering live coding, these libraries and all code in this cell have already been loaded\n",
    "import os\n",
    "current_work_dir = %pwd\n",
    "MLPy_ROOT = current_work_dir.split('course')[0]\n",
    "NOTEBOOK_FOLDER = os.path.join(MLPy_ROOT, 'course',  'notebooks')\n",
    "MODELS_FOLDER = os.path.join(NOTEBOOK_FOLDER, 'models')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Set up plotting options for seaborn and matplotlib\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks') \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "\n",
    "# load from previous lessons\n",
    "cached_files = [\n",
    "    'ames_train_y.pickle','ames_test_y.pickle',\n",
    "    'ames_train_X.pickle','ames_test_X.pickle',\n",
    "    'predictors.pickle','ames_ols_all.pickle',\n",
    "    'ames_ridge.pickle','ames_lasso.pickle', \n",
    "    'ames_enet.pickle',\n",
    "    'ames_pcr.pickle', 'ames_plsr.pickle',\n",
    "    'ames_rf.pickle', 'ames_knn.pickle'\n",
    "]\n",
    "\n",
    "for file in cached_files:\n",
    "    with open(os.path.join(MODELS_FOLDER, file), 'rb') as f:\n",
    "        objectname = file.replace('.pickle', '')\n",
    "        exec(objectname + \" = pickle.load(f)\")\n",
    "        f.close()\n",
    "\n",
    "## \n",
    "def assess_model_fit(models,\n",
    "                     model_labels, \n",
    "                     datasetX, \n",
    "                     datasetY):\n",
    "    columns = ['RMSE', 'R2', 'MAE']\n",
    "    rows = model_labels\n",
    "    results = pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "    for i, method in enumerate(models):\n",
    "        tmp_dataset_X = datasetX\n",
    "        # while we build the model and predict on the log10Transformed \n",
    "        # sale price, we display the error in dollars as that makes more sense\n",
    "        y_pred=10**(method.predict(tmp_dataset_X))\n",
    "        results.iloc[i,0] = np.sqrt(mean_squared_error(10**(datasetY), y_pred))\n",
    "        results.iloc[i,1] = r2_score(10**(datasetY), y_pred)\n",
    "        results.iloc[i,2] = mean_absolute_error(10**(datasetY), y_pred)\n",
    "    return results.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fbd6877-a8cd-4a0a-bafd-231cd9edfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29743308-206c-435a-98a0-14edad3ceafc",
   "metadata": {},
   "source": [
    "## Gradient boosting and XGBoost\n",
    "\n",
    "Models like `XGBoost` can achieve high accuracy\n",
    "on a wide range of datasets, but they are complex\n",
    "models with a large number of hyperparameters\n",
    "governing their behaviour.\n",
    "\n",
    "With a large number of hyperparameters:\n",
    "\n",
    "- It can be difficult to \"manually tune\" models by inspecting\n",
    "  how performance changes across different values of the parameter\n",
    "- The search space gets very large: if you have $n_1$ values for one parameter, then $n_2$ and $n_3$ values for other parameters, a grid\n",
    "  search has to test $n_1 \\times n_2 \\times n_3$ parameter combinations\n",
    "\n",
    "To tune models with many hyperparameters, you can:\n",
    "\n",
    "* Use the university HPC (Artemis) to run a grid search. Since this works best when you can split a task into smaller individual jobs, you can construct\n",
    "  a grid of parameters, and for each individual parameter combination\n",
    "  (and possibly each cross validation fold!),\n",
    "  pass them to a script that fits the training data to that combination\n",
    "  and returns the score\n",
    "* Use a parameter tuning algorithm that can explore the search space automatically, and suggest/generate parameters that are closer\n",
    "  to the optimal parameters. Examples include [hyperopt](http://hyperopt.github.io/hyperopt/) and [optuna](https://optuna.org/)\n",
    "\n",
    "To demonstrate this, we will use the [hyperopt](http://hyperopt.github.io/hyperopt/) package to optimize the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c24c52-6b5c-4d1d-9126-8c26340b8994",
   "metadata": {},
   "source": [
    "## Automatic parameter tuning with hyperopt\n",
    "\n",
    "Tuning a model with `hyperopt` requires you to define the search\n",
    "space a bit differently to the grid search: instead of providing\n",
    "specific values, you have to define the random distributions\n",
    "to sample from during tuning.\n",
    "\n",
    "A simple way to do this is to just specify `uniform` distributions for any parameters that can take fractional values (floats) and `randint` distributions for parameters that require integers.\n",
    "\n",
    "The difficult part is determining a range that's reasonable for each parameter - this requires some understanding of the model and how each parameter is used.\n",
    "\n",
    "> ### ⚠️ Challenge: XGBoost parameters\n",
    "> Review the documentation about XGBoost's [parameters](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster).\n",
    "> Do the values defined here for the parameter search look sensible?\n",
    "> If you don't have enough knowledge to judge this yourself, where\n",
    "> could you look for guidance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38acf8c2-d8ca-4fc4-b4ca-b23d969a5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'max_depth': hp.randint(\"max_depth\", 1, 5),\n",
    "    'gamma': hp.uniform('gamma', 0, 1),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha', 0, 50),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda', 10,100),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0, 1),\n",
    "    'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n",
    "    'n_estimators': hp.randint(\"n_estimators\", 50, 1000),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0, .15),\n",
    "    'max_bin' : hp.randint(\"max_bin\", 50, 500)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45207f1-ce16-470a-a20c-aa7bac00b484",
   "metadata": {},
   "source": [
    "`hyperopt` also requires you to define a function to fit the model\n",
    "for each parameter combination, and return the error metric. This is a bit more manual work than previous models, where `sklearn` has\n",
    "tools to automatically choose parameters based on an error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e49738d-7d3d-4ef4-8d7f-d2cc1056ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(space):\n",
    "    model = xgb.XGBRegressor(**space, objective='reg:squarederror')\n",
    "    # NOTE: using early stopping like this to prevent overfitting\n",
    "    #   may require a \"validation\" dataset, separate from\n",
    "    #   both the training and test set. We just use the test\n",
    "    #   set here for illustration\n",
    "    evaluation = [(ames_test_X, ames_test_y)]\n",
    "    model.fit(ames_train_X, ames_train_y, \n",
    "              eval_set=evaluation, \n",
    "              eval_metric=\"rmse\",\n",
    "              early_stopping_rounds=100,\n",
    "              verbose=False)\n",
    "\n",
    "    #Obtain prediction and rmse score.\n",
    "    pred = model.predict(ames_train_X)\n",
    "    rmse = root_mean_squared_error(ames_train_y, pred)\n",
    "    \n",
    "    #Specify what the loss is for each model.\n",
    "    return {'loss': rmse, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b6632-693e-4058-a1c2-91891ff785d1",
   "metadata": {},
   "source": [
    "Once you've defined the search space and the tuning function, you can start running the algorithm to search for the optimal parameters.\n",
    "\n",
    "For this workshop, we will only run a small number of iterations - definitely not enough to properly explore the parameter space.\n",
    "\n",
    "More iterations require more computing resources and more time. As a guide, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669039fa-d793-4f55-b326-832c941945df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████| 2000/2000 [11:07<00:00,  3.00trial/s, best loss: 0.029975555369995882]\n",
      "{'colsample_bytree': 0.21717333480526305, 'gamma': 0.00013087736688291036, 'learning_rate': 0.11492620818012601, 'max_bin': 481, 'max_depth': 3, 'min_child_weight': 0.0041109489525955846, 'n_estimators': 951, 'reg_alpha': 0.022904856862194, 'reg_lambda': 83.27009836487424}\n"
     ]
    }
   ],
   "source": [
    "n_trials = 20\n",
    "trials = Trials()\n",
    "best = fmin(fn=hyperparameter_tuning,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=n_trials,\n",
    "            trials=trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d7c8aa-0ff6-42b7-b90d-d0fa652181ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': [0.21717333480526305],\n",
       " 'gamma': [0.00013087736688291036],\n",
       " 'learning_rate': [0.11492620818012601],\n",
       " 'max_bin': [481],\n",
       " 'max_depth': [3],\n",
       " 'min_child_weight': [0.0041109489525955846],\n",
       " 'n_estimators': [951],\n",
       " 'reg_alpha': [0.022904856862194],\n",
       " 'reg_lambda': [83.27009836487424]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid = {}\n",
    "for key, val in best.items():\n",
    "    best_grid[key] = [val]\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe45288-ac4c-4d29-9d75-75c229d2e624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
