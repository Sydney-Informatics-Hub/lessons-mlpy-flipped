<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>45-Xgboost.utf8</title>

<script src="site_libs/header-attrs-2.5.3/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/journal.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="lesson.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 61px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h2 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h3 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h4 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h5 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h6 {
  padding-top: 66px;
  margin-top: -66px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="setup.html">Setup</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Session 1
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="03-EDA.html">Exploratory Data Analysis</a>
    </li>
    <li>
      <a href="10-LinReg.html">Linear Regression</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Session 2
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="11-RidgeLassoElasticNet.html">Regularised regression. PCR and PLSR.</a>
    </li>
    <li>
      <a href="30-RF_knn.html">Random Forests and k-NN regression</a>
    </li>
    <li>
      <a href="45-Xgboost.html">Gradient boosting. Extreme Gradient Boosting (XGBoost)</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Session 3
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="50-Classification.html">Classification</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Session 4
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="50-Classification.html">Classification (finish)</a>
    </li>
    <li>
      <a href="90-Unsupervised.html">Unsupervised learning</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<!--
---
title: "Gradient boosting. XGBoost"
author: "Darya Vanichkina"
exercises: 30
questions:
- What are some newer approaches to ML?
- What are their pros and cons?
objectives:
- To appreciate that more advanced methods can have a much larger number of parameters to optimise
- This optimisation is best done iteratively on the HPC
keypoints: 
- Modern approaches such as GBM and XGBoost can drastically improve prediction performance
- Tuning hyperparameters is, however, computationally expensive 
source: Rmd
teaching: 60
exercises: 30
bibliography: references.bib
---
-->
<div id="gbm.-xgboost" class="section level3">
<h3>GBM. XGBoost</h3>
<pre class="python"><code># when delivering live coding, these libraries and all code in this cell have already been loaded
websiterendering = True

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pandas.api.types import CategoricalDtype
import statsmodels.api as sm
import seaborn as sns
import pickle

import sys
sys.path.insert(0, &#39;py-earth&#39;)
from pyearth import Earth

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, RobustScaler

from sklearn.utils import resample


# Set up plotting options for seaborn and matplotlib
sns.set_context(&#39;notebook&#39;) 
sns.set_style(&#39;ticks&#39;) 
%matplotlib inline
plt.rcParams[&#39;figure.figsize&#39;] = (9, 6)

# load from previous lessons
cached_files = [&#39;models/ames_train_y.pickle&#39;,&#39;models/ames_test_y.pickle&#39;,
                &#39;models/ames_train_X.pickle&#39;,&#39;models/ames_test_X.pickle&#39;,
                &#39;models/predictors.pickle&#39;,&#39;models/ames_ols_all.pickle&#39;,
                &#39;models/ames_ridge.pickle&#39;,&#39;models/ames_lasso.pickle&#39;, 
                &#39;models/ames_enet.pickle&#39;,&#39;models/ames_mars.pickle&#39;,
               &#39;models/ames_pcr.pickle&#39;, &#39;models/ames_plsr.pickle&#39;,
               &#39;models/ames_RF.pickle&#39;, &#39;models/ames_knn.pickle&#39;]

for file in cached_files:
    with open(file, &#39;rb&#39;) as f:
        objectname = file.replace(&#39;models/&#39;, &#39;&#39;).replace(&#39;.pickle&#39;, &#39;&#39;)
        exec(objectname + &quot; = pickle.load(f)&quot;)
        f.close()

## 
def assess_model_fit(models,
                     model_labels, 
                     datasetX, 
                     datasetY):
    columns = [&#39;RMSE&#39;, &#39;R2&#39;, &#39;MAE&#39;]
    rows = model_labels
    results = pd.DataFrame(0.0, columns=columns, index=rows)
    for i, method in enumerate(models):
        tmp_dataset_X = datasetX
        # while we build the model and predict on the log10Transformed 
        # sale price, we display the error in dollars as that makes more sense
        y_pred=10**(method.predict(tmp_dataset_X))
        results.iloc[i,0] = np.sqrt(mean_squared_error(10**(datasetY), y_pred))
        results.iloc[i,1] = r2_score(10**(datasetY), y_pred)
        results.iloc[i,2] = mean_absolute_error(10**(datasetY), y_pred)
    return results.round(3)</code></pre>
<pre class="python"><code># these need to be loaded for this section
import itertools
from sklearn.ensemble import GradientBoostingRegressor  # GBM algorithm
import xgboost as xgb</code></pre>
</div>
<div id="gradient-boosting" class="section level2">
<h2>Gradient boosting</h2>
<p>Unlike algorithms we’ve worked with up to now, GBM has a substantially larger number of parameters. While we could optimise them all together using a GridSearchCV(), it makes more sense to do so iteratively, honing down on the best parameter sets one by one.</p>
<p>First, let’s take a <code>learning_rate</code> of 0.1, and identify how many <code>n_estimators</code> we’d use to fit the model. To do this:</p>
<pre class="python"><code># param_test1 = {&#39;n_estimators&#39;: list(range(20, len(predictors)+1, 10))}

# gbm1 = GridSearchCV(estimator=GradientBoostingRegressor(
#        loss=&#39;ls&#39;,
#        learning_rate=0.1, 
#        min_samples_split=20, 
#        min_samples_leaf=10,
#        max_depth=7, 
#        max_features=&#39;sqrt&#39;, 
#        subsample=0.8,   
#        random_state=42),
#    param_grid=param_test1, 
#    n_jobs=4, 
#    iid=False,
#    cv=10)
# gbm1.fit(ames_train_X, ames_train_y)
# pickle.dump(gbm1, open(&#39;models/gbm1.pickle&#39;, &#39;wb&#39;))

with open(&#39;models/gbm1.pickle&#39;, &#39;rb&#39;) as f:
    gbm1 = pickle.load(f)

# print(gsearch1.grid_scores_)
print(gbm1.best_params_)
print(gbm1.best_score_)</code></pre>
<pre><code>{&#39;n_estimators&#39;: 280}
0.9068897670500741</code></pre>
<pre class="python"><code># How many estimators did we test?
#print(param_test1)
#print(len(predictors))


# 
list(np.arange(0.001, 0.31, 0.05).round(2))</code></pre>
<pre><code>[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]</code></pre>
<p>We can see that the optimum number was found to be 280 (almost all of them!) - but we only tested 270 and 286. Perhaps the number is between these two? :et’s see if we can find out:</p>
<pre class="python"><code>param_test1 = {&#39;n_estimators&#39;: list(range(270, len(predictors)+1, 1))}

# gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(
#        loss=&#39;ls&#39;,
#        learning_rate=0.1, 
#        min_samples_split=20, 
#        min_samples_leaf=10,
#        max_depth=7, 
#        max_features=&#39;sqrt&#39;, 
#        subsample=0.8,   
#        random_state=42),
#    param_grid=param_test1, 
#    n_jobs=4, 
#    iid=False, 
#    cv=10)
# gbm2.fit(ames_train_X, ames_train_y)


# print(gbm2.best_params_)
# print(gbm2.best_score_)
print(param_test1)</code></pre>
<pre><code>{&#39;n_estimators&#39;: [270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286]}</code></pre>
<p>It turns out that with a <code>learning_rate</code> of 0.1, the optimal number of <code>n_estimators</code> is 280. (From this point, we start running things on the HPC, to achieve faster optimisation rates. However, this also means that we may not be able to match our pickles with those from our desktop, due to incompatibility of python package versions.</p>
<p>The next step is to optimise tree-specific parameters, starting with <code>max_depth</code> and <code>min_samples_split</code>. These correspond to:</p>
<ul>
<li><code>min_samples_split</code> : The minimum number of samples required to split an internal node. Values around 0.5-1% of the dataset can be feasible
<ul>
<li>We will test values of 5 to 30, in increments of 5</li>
</ul></li>
<li><code>max_depth</code> : maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree.
<ul>
<li>We will test values of 3 to 15, in increments of 2</li>
</ul></li>
</ul>
<p>This ends up being quite an expansive grid search, as we need to fit 6 * 7 = 42 models!</p>
<pre class="python"><code>param_test2 = {&#39;min_samples_split&#39;: list(range(5, 31, 5)),
               &#39;max_depth&#39;: list(range(3,16, 2))}</code></pre>
<p>To do this, we can use Artemis, the university’s HPC cluster. A simple way to do this is to fit each of the <code>min_samples_split</code> independently, using a python and bash script. The python script we’d use would look like this: It would write out the best model to a tsv file, with the corresponding paramter, after which we could use Unix to look at them.</p>
<pre><code># Import libraries:
import os
os.chdir(&#39;/scratch/RDS-LALA/darya/gbm&#39;)

import argparse
import numpy as np
import pandas as pd
import pickle
from sklearn.ensemble import GradientBoostingRegressor  # GBM algorithm
from sklearn import cross_validation, metrics  # Additional scklearn functions
from sklearn.grid_search import GridSearchCV  # Perforing grid search



parser = argparse.ArgumentParser(description=&#39;GBM model fitting&#39;)
parser.add_argument(&#39;min_samples_split&#39;, type=int, help=&#39;min_samples_split value to test&#39;)
args = parser.parse_args()




# load from previous lessons
cached_files = [&#39;models/ames_train_y.pickle&#39;,&#39;models/ames_test_y.pickle&#39;,
                &#39;models/ames_train_X.pickle&#39;,&#39;models/ames_test_X.pickle&#39;,
                &#39;models/predictors.pickle&#39;]

for file in cached_files:
    with open(file, &#39;rb&#39;) as f:
        objectname = file.replace(&#39;models/&#39;, &#39;&#39;).replace(&#39;.pickle&#39;, &#39;&#39;)
        exec(objectname + &quot; = pickle.load(f)&quot;)
        f.close()




param_test = {&#39;min_samples_split&#39;: [args.min_samples_split],
              &#39;max_depth&#39;: list(range(3,16, 2))}

gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(
       loss=&#39;ls&#39;,
       learning_rate=0.1,
       n_estimators = 279,
       min_samples_leaf=10,
       max_features=&#39;sqrt&#39;,
       subsample=0.8,
       random_state=42),
   param_grid=param_test,
   n_jobs=4,
   iid=False,
   cv=10)
gbm2.fit(ames_train_X, ames_train_y)

filename = &#39;fit1/&#39; + str(args.min_samples_split) + &#39;fit.tsv&#39;
with open(filename, &#39;w&#39;) as f:
    print(gbm2.best_score_,&quot;\t&quot;,gbm2.best_params_, file = f)
</code></pre>
<p>The shell script we could use is very basic:</p>
<pre><code>python /home/darya/gbm/optimise1.py $PARAM</code></pre>
<p>And, finally, we could execute it using the following qsub commands:</p>
<pre><code>qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=5 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh
qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=10 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh
qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=15 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh
qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=20 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh
qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=25 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh
qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=30 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh</code></pre>
<p>The results for this are as follows.</p>
<pre><code>0.9107460935801089   {&#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 30}</code></pre>
<p>It’s clear that we need to test higher values of the <code>min_samples_split</code>, so we can run another round of the optimisation with:</p>
<pre><code>qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=35 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh
qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=40 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh
qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=45 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh
qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=50 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh
qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=55 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh</code></pre>
<p>We then arrive at the following optimum:</p>
<pre><code>0.9109466273291733   {&#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 40}</code></pre>
<p>To improve the fit of our model, let’s try <code>max_depth</code> of 6, 7, or 8 and <code>min_samples_split</code> of 35 to 45 (increments of 1).</p>
<pre><code>param_test = {&#39;min_samples_split&#39;: list(range(35,46,1)),
              &#39;max_depth&#39;: list(range(6,9, 1))}</code></pre>
<p>We finally arrive at the optimum of:</p>
<pre><code>0.9118552463171431   {&#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 41}</code></pre>
<hr />
<p>Let’s leave <code>max_depth</code> at 7, and test a range of values for: - <code>min_samples_split</code> (35 to 45, increments of 1) - <code>min_samples_leaf</code> (5 to 45, increments of 5)</p>
<p>We can use the HPC, as above, to run the code, or run it locally on our machine (takes ~20 mins):</p>
<pre><code>param_test = {&#39;min_samples_split&#39;: list(range(35,46,1)),
              &#39;min_samples_leaf&#39;: list(range(5,36, 5))}

gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(
       loss=&#39;ls&#39;,
       learning_rate=0.1, 
       n_estimators = 279,
       max_depth=7, 
       max_features=&#39;sqrt&#39;, 
       subsample=0.8,   
       random_state=42),
   param_grid=param_test, 
   n_jobs=4, 
   iid=False, 
   cv=10)
gbm2.fit(ames_train_X, ames_train_y)

print(gbm2.best_score_,&quot;\t&quot;,gbm2.best_params_)</code></pre>
<p>The outcome of this is</p>
<pre><code>0.9151989049335256   {&#39;min_samples_leaf&#39;: 5, &#39;min_samples_split&#39;: 36}</code></pre>
<hr />
<p>Next, let’s tune the <code>max_features</code> parameter, trying values from ‘auto’ to ‘log2’ to ‘sqrt’ to only 20 parameters.</p>
<pre><code>param_test = {&#39;max_features&#39;: [&#39;auto&#39;,&#39;log2&#39;,&#39;sqrt&#39;],
              &#39;min_samples_leaf&#39;: list(range(5,21, 5))}

gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(
       loss=&#39;ls&#39;,
       min_samples_split = 36,
       learning_rate=0.1, 
       n_estimators = 279,
       max_depth=7,  
       subsample=0.8,   
       random_state=42),
   param_grid=param_test, 
   n_jobs=4, 
   iid=False, 
   cv=10)
gbm2.fit(ames_train_X, ames_train_y)

print(gbm2.best_score_,&quot;\t&quot;,gbm2.best_params_)</code></pre>
<p>The resulf of this is:</p>
<p><code>0.9151989049335256      {'min_samples_leaf': 5, 'max_features': 'sqrt'}</code></p>
<hr />
<p>Finally, let’s go back to optimising the number of trees and learning rate, this time going with a lower learning rate (because the lower the learning rate, the slower the CV will be):</p>
<pre><code>param_test = {&#39;n_estimators&#39;: list(range(275, len(predictors)+1, 1)),
             &#39;learning_rate&#39;: list(np.arange(0.001, 1.0, 0.1)),
             &#39;subsample&#39;: list(np.arange(0.6, 1.0))}

gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(
       loss=&#39;ls&#39;,
       max_depth=7, 
       max_features=&#39;sqrt&#39;, 
       min_samples_leaf=5,
       min_samples_split = 36,
       random_state=42),
   param_grid=param_test, 
   n_jobs=4, 
   iid=False, 
   cv=10)
gbm2.fit(ames_train_X, ames_train_y)

print(gbm2.best_score_,&quot;\t&quot;,gbm2.best_params_)
</code></pre>
<p><code>0.9117722351540429      {'learning_rate': 0.101, 'n_estimators': 280, 'subsample': 0.6}</code></p>
<p>Note that we could also have “brute forced” the estimation using HPC, combining 3 scripts:</p>
<pre><code># optimise2.py ----------------
#
#
# Import libraries:
import os
os.chdir(&#39;/scratch/RDS-CORE-SIH4HPC-RW/darya/gbm&#39;)

import argparse
import numpy as np
import pandas as pd
import pickle
from sklearn.ensemble import GradientBoostingRegressor  # GBM algorithm
from sklearn import cross_validation, metrics  # Additional scklearn functions
from sklearn.grid_search import GridSearchCV  # Perforing grid search



parser = argparse.ArgumentParser(description=&#39;GBM model fitting&#39;)
parser.add_argument(&#39;min_samples_split&#39;, type=int, help=&#39;min_samples_split value to test&#39;)
parser.add_argument(&#39;n_estimators&#39;, type=int, help=&#39;n_estimators value to test&#39;)
args = parser.parse_args()


# load from previous lessons
cached_files = [&#39;models/ames_train_y.pickle&#39;,&#39;models/ames_test_y.pickle&#39;,
                &#39;models/ames_train_X.pickle&#39;,&#39;models/ames_test_X.pickle&#39;,
                &#39;models/predictors.pickle&#39;]

for file in cached_files:
    with open(file, &#39;rb&#39;) as f:
        objectname = file.replace(&#39;models/&#39;, &#39;&#39;).replace(&#39;.pickle&#39;, &#39;&#39;)
        exec(objectname + &quot; = pickle.load(f)&quot;)
        f.close()



# [args.min_samples_split]

param_test = {&#39;n_estimators&#39;: [args.n_estimators], 
            &#39;min_samples_split&#39; : [args.min_samples_split], 
             &#39;learning_rate&#39;: list(np.arange(0.001, 0.31, 0.05).round(2)), 
             &#39;min_samples_leaf&#39;:  list(range(1,22, 4)),
             &#39;max_depth&#39;: [1,3,5,6,7,8,9,10,15],
             &#39;subsample&#39;: [0.65, 0.75, 0.8, 0.85, 0.9]
             }

gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(
       loss=&#39;ls&#39;,
       max_features=&#39;sqrt&#39;,
       random_state=42),
   param_grid=param_test,
   n_jobs=4,
   iid=False,
   cv=10)


gbm2.fit(ames_train_X, ames_train_y)

filename = &#39;fit1/&#39; + str(args.min_samples_split) + &quot;_&quot; + str(args.n_estimators) + &#39;fit.tsv&#39;
with open(filename, &#39;w&#39;) as f:
    print(gbm2.best_score_,&quot;\t&quot;,gbm2.best_params_, file = f)
    
    
    
    

####----
optimise2.pbs
#PBS -P PROJECTNAME
#PBS -N gbmOpt3
#PBS -l select=1:ncpus=3:mem=2gb
#PBS -l walltime=02:0:00
#PBS -J 100-286:5

# unload all modules
module purge

cd /home/dvanichkina/scratch_sih/darya/gbm/
python /home/dvanichkina/scratch_sih/darya/gbm/optimise2.py $PARAM $PBS_ARRAY_INDEX



#### --------------
# optimise2.sh
qsub -v PARAM=5 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs
qsub -v PARAM=10 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs
qsub -v PARAM=15 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs
qsub -v PARAM=20 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs
qsub -v PARAM=25 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs
qsub -v PARAM=30 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs
qsub -v PARAM=35 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs
qsub -v PARAM=40 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs
qsub -v PARAM=45 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs
</code></pre>
<p>After all of this finishes running, the optimal parameter values we obtain are:</p>
<pre><code>0.918410561940631    {&#39;learning_rate&#39;: 0.051, &#39;n_estimators&#39;: 280, &#39;min_samples_split&#39;: 35, &#39;max_depth&#39;: 7, &#39;subsample&#39;: 0.8, &#39;min_samples_leaf&#39;: 1}</code></pre>
<p>Let’s fit this model locally:</p>
<pre class="python"><code>param_test = {&#39;n_estimators&#39;: [280]}

ames_gbm = GridSearchCV(estimator=GradientBoostingRegressor(
        loss=&#39;ls&#39;,
        learning_rate=0.051, 
        min_samples_split=35, 
        min_samples_leaf=1,
        max_depth=7, 
        max_features=&#39;sqrt&#39;, 
        subsample=0.8,   
        random_state=42),
    param_grid=param_test, 
    n_jobs=4, 
    iid=False,
    cv=10)

#ames_gbm.fit(ames_train_X, ames_train_y)
#pickle.dump(ames_gbm, open(&#39;models/ames_gbm.pickle&#39;, &#39;wb&#39;))

with open(&#39;models/ames_gbm.pickle&#39;, &#39;rb&#39;) as f:
    ames_gbm = pickle.load(f)

# print(gsearch1.grid_scores_)
print(ames_gbm.best_params_)
print(ames_gbm.best_score_)</code></pre>
<pre><code>{&#39;n_estimators&#39;: 280}
0.9182771422480472</code></pre>
<hr />
</div>
<div id="xgboost" class="section level2">
<h2>XGBoost</h2>
<ol style="list-style-type: decimal">
<li>First, optimise <code>max_depth</code> and <code>min_child_weight</code>, with a varying number of <code>n_estimators</code> and a fixed, large <code>learning_rate</code> 0.1.</li>
</ol>
<pre><code>import numpy as np
import pandas as pd
import pickle
import xgboost as xgb
from sklearn import cross_validation, metrics  # Additional scklearn functions
from sklearn.grid_search import GridSearchCV  # Perforing grid search
# load from previous lessons
cached_files = [&#39;models/ames_train_y.pickle&#39;,&#39;models/ames_test_y.pickle&#39;,
                &#39;models/ames_train_X.pickle&#39;,&#39;models/ames_test_X.pickle&#39;,
                &#39;models/predictors.pickle&#39;]

for file in cached_files:
    with open(file, &#39;rb&#39;) as f:
        objectname = file.replace(&#39;models/&#39;, &#39;&#39;).replace(&#39;.pickle&#39;, &#39;&#39;)
        exec(objectname + &quot; = pickle.load(f)&quot;)
        f.close()




param_test = {
 &#39;max_depth&#39;:list(range(3,10,2)),
 &#39;min_child_weight&#39;:list(range(1,7,2)),
 &#39;n_estimators&#39;: [180, 230, 280]
}



xgbmodel = GridSearchCV(estimator=xgb.XGBRegressor(
  objective=&#39;reg:linear&#39;,
  learning_rate =0.1,
  n_estimators=280,
  gamma=0,
  subsample=0.8,
  colsample_bytree=0.8,
  random_state=42),
   param_grid=param_test,
   n_jobs=4,
   iid=False,
   cv=10)
   
xgbmodel.fit(ames_train_X, ames_train_y)
print(xgbmodel.best_score_,&quot;\t&quot;,xgbmodel.best_params_)

# 0.9159701741062035     {&#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 1, &#39;n_estimators&#39;: 280}</code></pre>
<p>Of these, it turns out the best parameter values are: - ‘max_depth’: 3, ‘min_child_weight’: 1, ‘n_estimators’: 280</p>
<p>Now, let’s see if we can improve this even more:</p>
<pre><code>param_test = {
 &#39;max_depth&#39;:[2,3,4],
 &#39;min_child_weight&#39;:[0.5,1,1.5,2],
 &#39;n_estimators&#39;: [275, 280, 285]
}

# 0.9162060722720288     {&#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 0.5, &#39;n_estimators&#39;: 285}
</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Next, optimise gamma, the minimum split loss. This is the minimum loss reduction required to make a further partition on a leaf node of the tree.</li>
</ol>
<pre><code>param_test = {
 &#39;max_depth&#39;:[3],
 &#39;min_child_weight&#39;:[0.5],
 &#39;n_estimators&#39;: [283, 284, 285],
 &#39;gamma&#39;: [i/10.0 for i in range(0,5)]
}

# 0.9162060722720288     {&#39;gamma&#39;: 0.0, &#39;min_child_weight&#39;: 0.5, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 285}
# Default zero seems best
</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Next, optimise <code>subsample</code> and <code>colsample_bytree</code>, which represent the proportion of data used to make each tree and how many features can go into a tree at each branch</li>
</ol>
<pre><code>param_test = {
 &#39;max_depth&#39;:[3],
 &#39;min_child_weight&#39;:[0.5],
 &#39;n_estimators&#39;: [283, 284, 285, 286],
 &#39;gamma&#39;: [0],
 &#39;subsample&#39;:[i/10.0 for i in range(6,10)],
 &#39;colsample_bytree&#39;:[i/10.0 for i in range(6,10)]

}


# 0.916463171211643      {&#39;max_depth&#39;: 3, &#39;gamma&#39;: 0, &#39;min_child_weight&#39;: 0.5, &#39;colsample_bytree&#39;: 0.6, &#39;subsample&#39;: 0.8, &#39;n_estimators&#39;: 284}
</code></pre>
<p>Let’s zero in on that, and subsample values plus/minus 0.05 around the idenfied optima:</p>
<pre><code>param_test = {
 &#39;max_depth&#39;:[3],
 &#39;min_child_weight&#39;:[0.5],
 &#39;n_estimators&#39;: [283, 284, 285, 286],
 &#39;gamma&#39;: [0],
 &#39;subsample&#39;:[0.75, 0.775,  0.8, 0.825, 0.85],
 &#39;colsample_bytree&#39;:[0.5, 0.55, 0.6]

}


# 0.918366305109824      {&#39;max_depth&#39;: 3, &#39;gamma&#39;: 0, &#39;min_child_weight&#39;: 0.5, &#39;colsample_bytree&#39;: 0.5, &#39;subsample&#39;: 0.75, &#39;n_estimators&#39;: 285}
</code></pre>
<p>Perhaps need to reduce those two even more, as their values are the lowest that we’re testing!</p>
<pre><code>
param_test = {
 &#39;max_depth&#39;:[3],
 &#39;min_child_weight&#39;:[0.5],
 &#39;n_estimators&#39;: [283, 284, 285, 286],
 &#39;gamma&#39;: [0],
 &#39;subsample&#39;:[0.65, 0.7, 0.75, 0.8],
 &#39;colsample_bytree&#39;:[0.4,0.45, 0.5, 0.55]

}
# 0.918366305109824      {&#39;max_depth&#39;: 3, &#39;gamma&#39;: 0, &#39;min_child_weight&#39;: 0.5, &#39;colsample_bytree&#39;: 0.5, &#39;subsample&#39;: 0.75, &#39;n_estimators&#39;: 285}</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Next, let’s tune the regularisation parameters:</li>
</ol>
<pre><code>param_test = {
 &#39;max_depth&#39;:[3],
 &#39;min_child_weight&#39;:[0.5],
 &#39;n_estimators&#39;: [285],
 &#39;gamma&#39;: [0],
 &#39;subsample&#39;:[0.75],
 &#39;colsample_bytree&#39;:[0.5],
 &#39;reg_alpha&#39;:[0, 0.1, 0.2, 0.5, 1],
 &#39;reg_lambda&#39;: [0, 0.1, 0.2, 0.5, 1]
}

# 0.918366305109824      {&#39;max_depth&#39;: 3, &#39;reg_lambda&#39;: 1, &#39;reg_alpha&#39;: 0, &#39;gamma&#39;: 0, &#39;min_child_weight&#39;: 0.5, &#39;colsample_bytree&#39;: 0.5, &#39;subsample&#39;: 0.75, &#39;n_estimators&#39;: 285}

# defaults we&#39;ve been using so far: alpha = 0 , lambda = 1, are optimal for us (i.e. ridge regression-like)</code></pre>
<p>Let’s fit the final model, and assess its performance on the training and test sets:</p>
<pre class="python"><code>param_test = {
 &#39;max_depth&#39;:[3],
 &#39;min_child_weight&#39;:[0.5],
 &#39;n_estimators&#39;: [285],
 &#39;gamma&#39;: [0],
 &#39;subsample&#39;:[0.75],
 &#39;colsample_bytree&#39;:[0.5]
}

if websiterendering:
    with open(&#39;models/ames_xgb.pickle&#39;, &#39;rb&#39;) as f:
        ames_xgb = pickle.load(f)
else:
    # STUDENTS: RUN THE LINE BELOW ONLY:
    ames_xgb = GridSearchCV(estimator=xgb.XGBRegressor(
    objective=&#39;reg:linear&#39;,
    learning_rate =0.1,
    random_state=42),
    param_grid=param_test,
    n_jobs=4,
    iid=False,
    cv=10)
    ames_xgb.fit(ames_train_X, ames_train_y)
    pickle.dump(ames_xgb, open(&#39;models/ames_xgb.pickle&#39;, &#39;wb&#39;))
    


# print(gsearch1.grid_scores_)
print(ames_xgb.best_params_)
print(ames_xgb.best_score_)</code></pre>
<pre><code>[17:11:24] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1572314959925/work/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
{&#39;colsample_bytree&#39;: 0.5, &#39;gamma&#39;: 0, &#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 0.5, &#39;n_estimators&#39;: 285, &#39;subsample&#39;: 0.75}
0.9113828044927915</code></pre>
<p>Unfortunately, even the code above runs out of RAM on my normal machine, so I optimised it all on Artemis HPC. I then ran code very similar to what we’ve used before to get the training and testing RMSE.</p>
<pre class="python"><code># What was the RMSE on the training data?
assess_model_fit(models=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, ames_pcr, ames_plsr, ames_mars, ames_RF, ames_knn, ames_gbm, ames_xgb],
    model_labels=[&#39;OLS&#39;,&#39;Ridge&#39;, &#39;Lasso&#39;, &#39;ENet&#39;,&#39;PCR&#39;,&#39;PLSR&#39;,&#39;MARS&#39;, &#39;RF&#39;, &#39;kNN&#39;, &#39;GB&#39;, &quot;XGB&quot;],
    datasetX=ames_train_X,
    datasetY=ames_train_y).sort_values(&quot;RMSE&quot;)</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
RMSE
</th>
<th>
R2
</th>
<th>
MAE
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
GB
</th>
<td>
11528.128
</td>
<td>
0.979
</td>
<td>
7274.574
</td>
</tr>
<tr>
<th>
XGB
</th>
<td>
11755.607
</td>
<td>
0.978
</td>
<td>
8566.560
</td>
</tr>
<tr>
<th>
OLS
</th>
<td>
15757.714
</td>
<td>
0.961
</td>
<td>
10931.668
</td>
</tr>
<tr>
<th>
RF
</th>
<td>
16094.641
</td>
<td>
0.959
</td>
<td>
11014.212
</td>
</tr>
<tr>
<th>
Lasso
</th>
<td>
16480.809
</td>
<td>
0.957
</td>
<td>
11448.829
</td>
</tr>
<tr>
<th>
Ridge
</th>
<td>
16497.181
</td>
<td>
0.957
</td>
<td>
11462.724
</td>
</tr>
<tr>
<th>
PLSR
</th>
<td>
16524.567
</td>
<td>
0.957
</td>
<td>
11602.496
</td>
</tr>
<tr>
<th>
PCR
</th>
<td>
16741.308
</td>
<td>
0.956
</td>
<td>
11753.909
</td>
</tr>
<tr>
<th>
ENet
</th>
<td>
17041.271
</td>
<td>
0.954
</td>
<td>
11799.798
</td>
</tr>
<tr>
<th>
MARS
</th>
<td>
19172.923
</td>
<td>
0.942
</td>
<td>
13498.476
</td>
</tr>
<tr>
<th>
kNN
</th>
<td>
31651.974
</td>
<td>
0.841
</td>
<td>
20155.938
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code># What was the RMSE on the test data?

assess_model_fit(models=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, ames_pcr, ames_plsr, ames_mars, ames_RF, ames_knn, ames_gbm, ames_xgb],
                 model_labels=[&#39;OLS&#39;,&#39;Ridge&#39;, &#39;Lasso&#39;, &#39;ENet&#39;,&#39;PCR&#39;,&#39;PLSR&#39;,&#39;MARS&#39;, &#39;RF&#39;, &#39;kNN&#39;, &#39;GB&#39;, &quot;XGB&quot;],
                 datasetX=ames_test_X,
                 datasetY=ames_test_y).sort_values(&quot;RMSE&quot;)</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
RMSE
</th>
<th>
R2
</th>
<th>
MAE
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
GB
</th>
<td>
19131.898
</td>
<td>
0.938
</td>
<td>
11399.676
</td>
</tr>
<tr>
<th>
ENet
</th>
<td>
19801.125
</td>
<td>
0.933
</td>
<td>
13317.465
</td>
</tr>
<tr>
<th>
Lasso
</th>
<td>
19864.493
</td>
<td>
0.933
</td>
<td>
13120.146
</td>
</tr>
<tr>
<th>
Ridge
</th>
<td>
20024.975
</td>
<td>
0.932
</td>
<td>
13270.709
</td>
</tr>
<tr>
<th>
PLSR
</th>
<td>
20113.237
</td>
<td>
0.931
</td>
<td>
13372.746
</td>
</tr>
<tr>
<th>
OLS
</th>
<td>
20541.485
</td>
<td>
0.928
</td>
<td>
13346.733
</td>
</tr>
<tr>
<th>
PCR
</th>
<td>
21139.689
</td>
<td>
0.924
</td>
<td>
13993.637
</td>
</tr>
<tr>
<th>
XGB
</th>
<td>
21230.108
</td>
<td>
0.923
</td>
<td>
13446.263
</td>
</tr>
<tr>
<th>
MARS
</th>
<td>
23226.020
</td>
<td>
0.908
</td>
<td>
15355.804
</td>
</tr>
<tr>
<th>
RF
</th>
<td>
27557.506
</td>
<td>
0.870
</td>
<td>
16899.533
</td>
</tr>
<tr>
<th>
kNN
</th>
<td>
34498.941
</td>
<td>
0.797
</td>
<td>
22983.686
</td>
</tr>
</tbody>
</table>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
